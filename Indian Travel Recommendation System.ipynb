{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üåçüöÄ Incredible India Explorer\n",
    "AI-Powered Indian Travel Recommender System\n",
    "### üìã Overview\n",
    "\n",
    "This notebook creates a comprehensive travel recommendation system using:\n",
    "- **Content-Based Filtering**: Recommends destinations based on features (type, tags, activities, etc.)\n",
    "- **Collaborative Filtering**: Recommends based on user behavior patterns\n",
    "- **Hybrid Approach**: Combines both methods for better recommendations\n",
    "\n",
    "### üìä Datasets Used\n",
    "\n",
    "1. **Destination_df.csv** - 10,000 destinations with 33 features\n",
    "2. **Users_df.csv** - 10,000 user profiles\n",
    "3. **Users_History_df.csv** - 12,275 trip records\n",
    "4. **Reviews_df.csv** - 10,000 reviews\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "================================================================================\n",
      "‚úÖ Users Dataset: (10000, 16)\n",
      "‚úÖ Destinations Dataset: (9964, 33)\n",
      "‚úÖ User History Dataset: (12275, 16)\n",
      "‚úÖ Reviews Dataset: (10000, 7)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load all datasets\n",
    "users_df = pd.read_csv('Users_df.csv')\n",
    "destinations_df = pd.read_csv('Destination_df.csv')\n",
    "history_df = pd.read_csv('Users_History_df.csv')\n",
    "reviews_df = pd.read_csv('Reviews_df.csv')\n",
    "\n",
    "print(f\"‚úÖ Users Dataset: {users_df.shape}\")\n",
    "print(f\"‚úÖ Destinations Dataset: {destinations_df.shape}\")\n",
    "print(f\"‚úÖ User History Dataset: {history_df.shape}\")\n",
    "print(f\"‚úÖ Reviews Dataset: {reviews_df.shape}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Destination Dataset Info:\n",
      "--------------------------------------------------------------------------------\n",
      "Total Destinations: 9,964\n",
      "\n",
      "Columns (33):\n",
      "   1. DestinationID                  (int64)\n",
      "   2. Name                           (object)\n",
      "   3. City                           (object)\n",
      "   4. State/UT                       (object)\n",
      "   5. Type                           (object)\n",
      "   6. Tags                           (object)\n",
      "   7. Popularity                     (float64)\n",
      "   8. RatingCount                    (int64)\n",
      "   9. BestTimeToVisit                (object)\n",
      "  10. WeatherSummary                 (object)\n",
      "  11. Latitude                       (object)\n",
      "  12. Longitude                      (float64)\n",
      "  13. EntryFee                       (object)\n",
      "  14. AverageCost                    (object)\n",
      "  15. RecommendedDuration            (object)\n",
      "  16. Activities                     (object)\n",
      "  17. Accessibility                  (object)\n",
      "  18. NearestAirport                 (object)\n",
      "  19. NearestRailwayStation          (object)\n",
      "  20. Description                    (object)\n",
      "  21. DestinationImage               (object)\n",
      "  22. Hotel1_Name                    (object)\n",
      "  23. Hotel1_Link                    (object)\n",
      "  24. Hotel1_Price                   (int64)\n",
      "  25. Hotel2_Name                    (object)\n",
      "  26. Hotel2_Link                    (object)\n",
      "  27. Hotel2_Price                   (int64)\n",
      "  28. Hotel3_Name                    (object)\n",
      "  29. Hotel3_Link                    (object)\n",
      "  30. Hotel3_Price                   (int64)\n",
      "  31. Hotel4_Name                    (object)\n",
      "  32. Hotel4_Link                    (object)\n",
      "  33. Hotel4_Price                   (int64)\n",
      "\n",
      "üìä Key Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Destination Types: 44\n",
      "States/UTs: 165\n",
      "Cities: 1996\n"
     ]
    }
   ],
   "source": [
    "# Display basic information\n",
    "print(\"\\nüîç Destination Dataset Info:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total Destinations: {len(destinations_df):,}\")\n",
    "print(f\"\\nColumns ({len(destinations_df.columns)}):\")\n",
    "for i, col in enumerate(destinations_df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col:<30} ({destinations_df[col].dtype})\")\n",
    "\n",
    "print(\"\\nüìä Key Statistics:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Destination Types: {destinations_df['Type'].nunique()}\")\n",
    "print(f\"States/UTs: {destinations_df['State/UT'].nunique()}\")\n",
    "print(f\"Cities: {destinations_df['City'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Missing Values Analysis:\n",
      "--------------------------------------------------------------------------------\n",
      "     Column  Missing Percentage\n",
      "       Name        0      0.00%\n",
      "       Type        0      0.00%\n",
      "       Tags        0      0.00%\n",
      "Description        0      0.00%\n",
      " Activities        0      0.00%\n",
      "       City        0      0.00%\n",
      "   State/UT        0      0.00%\n",
      " Popularity        0      0.00%\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in key columns\n",
    "print(\"\\nüîç Missing Values Analysis:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "key_cols = ['Name', 'Type', 'Tags', 'Description', 'Activities', 'City', 'State/UT', 'Popularity']\n",
    "missing_data = []\n",
    "\n",
    "for col in key_cols:\n",
    "    if col in destinations_df.columns:\n",
    "        missing = destinations_df[col].isna().sum()\n",
    "        pct = (missing / len(destinations_df)) * 100\n",
    "        missing_data.append({\n",
    "            'Column': col,\n",
    "            'Missing': missing,\n",
    "            'Percentage': f\"{pct:.2f}%\"\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Top 10 Destination Types:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. Nature                    :  1629 destinations\n",
      " 2. Adventure                 :  1421 destinations\n",
      " 3. Wildlife                  :   698 destinations\n",
      " 4. Heritage                  :   598 destinations\n",
      " 5. Religious                 :   564 destinations\n",
      " 6. Historical                :   512 destinations\n",
      " 7. Beach                     :   500 destinations\n",
      " 8. Museum                    :   434 destinations\n",
      " 9. Hill Station              :   433 destinations\n",
      "10. Temple                    :   418 destinations\n",
      "\n",
      "üó∫Ô∏è Top 10 States:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. West Bengal               :   661 destinations\n",
      " 2. Sikkim                    :   529 destinations\n",
      " 3. Ladakh                    :   504 destinations\n",
      " 4. Tamil Nadu                :   500 destinations\n",
      " 5. Himachal Pradesh          :   475 destinations\n",
      " 6. Uttarakhand               :   472 destinations\n",
      " 7. Maharashtra               :   422 destinations\n",
      " 8. Karnataka                 :   391 destinations\n",
      " 9. Kerala                    :   320 destinations\n",
      "10. Meghalaya                 :   308 destinations\n"
     ]
    }
   ],
   "source": [
    "# Display top destination types\n",
    "print(\"\\nüìä Top 10 Destination Types:\")\n",
    "print(\"-\"*80)\n",
    "top_types = destinations_df['Type'].value_counts().head(10)\n",
    "for i, (dtype, count) in enumerate(top_types.items(), 1):\n",
    "    print(f\"{i:2d}. {dtype:<25} : {count:>5} destinations\")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Top 10 States:\")\n",
    "print(\"-\"*80)\n",
    "top_states = destinations_df['State/UT'].value_counts().head(10)\n",
    "for i, (state, count) in enumerate(top_states.items(), 1):\n",
    "    print(f\"{i:2d}. {state:<25} : {count:>5} destinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Data Preprocessing...\n",
      "================================================================================\n",
      "\n",
      "1. Filling missing values...\n",
      "   ‚úÖ Missing values handled\n",
      "\n",
      "2. Sample preprocessed data:\n",
      "--------------------------------------------------------------------------------\n",
      "          Name        Type    City       State/UT\n",
      "0    Taj Mahal  Historical    Agra  Uttar Pradesh\n",
      "1   Amber Fort  Historical  Jaipur      Rajasthan\n",
      "2  Goa Beaches       Beach  Panaji            Goa\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüßπ Data Preprocessing...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a working copy\n",
    "df = destinations_df.copy()\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\n1. Filling missing values...\")\n",
    "df['Tags'] = df['Tags'].fillna('')\n",
    "df['Description'] = df['Description'].fillna('')\n",
    "df['Activities'] = df['Activities'].fillna('')\n",
    "df['Type'] = df['Type'].fillna('Unknown')\n",
    "df['City'] = df['City'].fillna('')\n",
    "df['State/UT'] = df['State/UT'].fillna('')\n",
    "print(\"   ‚úÖ Missing values handled\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n2. Sample preprocessed data:\")\n",
    "print(\"-\"*80)\n",
    "print(df[['Name', 'Type', 'City', 'State/UT']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Feature Engineering for Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Feature Engineering...\n",
      "================================================================================\n",
      "\n",
      "Creating combined content features...\n",
      "‚úÖ Content features created!\n",
      "\n",
      "Sample feature string (first 300 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "HistoricalHistoricalHistorical heritage, architecture, culture, monument, guided tour, photography, historic landmark, iconic, white, marble, tourist favorite, must visit, photogenicheritage, architecture, culture, monument, guided tour, photography, historic landmark, iconic, white, marble, tourist...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ Feature Engineering...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_content_features(row):\n",
    "    \"\"\"\n",
    "    Combine multiple columns to create rich feature representation\n",
    "    \n",
    "    Weighting strategy:\n",
    "    - Type: 3x (most important - primary category)\n",
    "    - Tags: 2x (important - detailed characteristics)\n",
    "    - Activities: 1x (standard weight)\n",
    "    - Description: 1x (limited to 200 chars to avoid noise)\n",
    "    - State/UT: 1x (for regional similarity)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Type (weighted 3x)\n",
    "    features.append(str(row['Type']) * 3)\n",
    "    \n",
    "    # Tags (weighted 2x)\n",
    "    features.append(str(row['Tags']) * 2)\n",
    "    \n",
    "    # Activities\n",
    "    features.append(str(row['Activities']))\n",
    "    \n",
    "    # Description (truncated)\n",
    "    desc = str(row['Description'])[:200]\n",
    "    features.append(desc)\n",
    "    \n",
    "    # State/UT (regional similarity)\n",
    "    features.append(str(row['State/UT']))\n",
    "    \n",
    "    return ' '.join(features)\n",
    "\n",
    "print(\"\\nCreating combined content features...\")\n",
    "df['content_features'] = df.apply(create_content_features, axis=1)\n",
    "\n",
    "print(\"‚úÖ Content features created!\")\n",
    "print(f\"\\nSample feature string (first 300 chars):\")\n",
    "print(\"-\"*80)\n",
    "print(df['content_features'].iloc[0][:300] + \"...\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Build TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Building TF-IDF Vectorizer...\n",
      "================================================================================\n",
      "\n",
      "Fitting TF-IDF vectorizer...\n",
      "\n",
      "‚úÖ TF-IDF Matrix Created!\n",
      "   Shape: (9964, 5000)\n",
      "   (9,964 destinations √ó 5,000 features)\n",
      "   Sparsity: 99.26%\n",
      "   Memory: ~2.82 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Building TF-IDF Vectorizer...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with optimized parameters\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,        # Limit to top 5000 features (memory efficient)\n",
    "    stop_words='english',     # Remove common English words\n",
    "    ngram_range=(1, 2),       # Use both unigrams and bigrams\n",
    "    min_df=2,                 # Ignore terms appearing in < 2 documents\n",
    "    max_df=0.8                # Ignore terms appearing in > 80% documents\n",
    ")\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "tfidf_matrix = tfidf.fit_transform(df['content_features'])\n",
    "\n",
    "print(f\"\\n‚úÖ TF-IDF Matrix Created!\")\n",
    "print(f\"   Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   ({tfidf_matrix.shape[0]:,} destinations √ó {tfidf_matrix.shape[1]:,} features)\")\n",
    "print(f\"   Sparsity: {(1.0 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"   Memory: ~{tfidf_matrix.data.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 7: Compute Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Computing Cosine Similarity Matrix...\n",
      "================================================================================\n",
      "‚è≥ This may take a few minutes for large datasets...\n",
      "\n",
      "\n",
      "‚úÖ Cosine Similarity Matrix Created!\n",
      "   Shape: (9964, 9964)\n",
      "   (9,964 √ó 9,964)\n",
      "   Memory: ~757.46 MB\n",
      "\n",
      "üìà Sample Similarities for 'Taj Mahal':\n",
      "--------------------------------------------------------------------------------\n",
      "   Taj Mahal                                : 1.0000\n",
      "   Amber Fort                               : 0.2591\n",
      "   Goa Beaches                              : 0.0115\n",
      "   Alleppey Backwaters                      : 0.0259\n",
      "   Pangong Lake                             : 0.0188\n",
      "   Varanasi Ghats                           : 0.0516\n",
      "   Hawa Mahal                               : 0.4146\n",
      "   Munnar Tea Gardens                       : 0.0198\n",
      "   Khajuraho Temples                        : 0.2666\n",
      "   Ajanta Ellora Caves                      : 0.3022\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Computing Cosine Similarity Matrix...\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚è≥ This may take a few minutes for large datasets...\\n\")\n",
    "\n",
    "# Use linear_kernel (faster than cosine_similarity for TF-IDF)\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Cosine Similarity Matrix Created!\")\n",
    "print(f\"   Shape: {cosine_sim.shape}\")\n",
    "print(f\"   ({cosine_sim.shape[0]:,} √ó {cosine_sim.shape[1]:,})\")\n",
    "print(f\"   Memory: ~{cosine_sim.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show some sample similarities\n",
    "print(f\"\\nüìà Sample Similarities for '{df['Name'].iloc[0]}':\")\n",
    "print(\"-\"*80)\n",
    "sample_sims = cosine_sim[0][:10]\n",
    "for i, sim in enumerate(sample_sims):\n",
    "    print(f\"   {df['Name'].iloc[i]:<40} : {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 8: Create Indices Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÇÔ∏è Creating Indices Mapping...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Indices Mapping Created!\n",
      "   Total mappings: 9,866\n",
      "\n",
      "Sample mappings:\n",
      "--------------------------------------------------------------------------------\n",
      "   Taj Mahal                                -> Index 0\n",
      "   Amber Fort                               -> Index 1\n",
      "   Goa Beaches                              -> Index 2\n",
      "   Alleppey Backwaters                      -> Index 9925\n",
      "   Pangong Lake                             -> Index 9908\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüóÇÔ∏è Creating Indices Mapping...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create mapping: destination name -> dataframe index\n",
    "indices = pd.Series(df.index, index=df['Name']).to_dict()\n",
    "\n",
    "print(f\"\\n‚úÖ Indices Mapping Created!\")\n",
    "print(f\"   Total mappings: {len(indices):,}\")\n",
    "print(f\"\\nSample mappings:\")\n",
    "print(\"-\"*80)\n",
    "for i, (name, idx) in enumerate(list(indices.items())[:5]):\n",
    "    print(f\"   {name:<40} -> Index {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 9: Test Content-Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Testing Content-Based Recommendations:\n",
      "================================================================================\n",
      "\n",
      "üéØ Getting recommendations for: Taj Mahal\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 Similar Destinations:\n",
      "                           Name    City       State/UT       Type  Popularity  Similarity_Score\n",
      "                  Jaswant Thada Jodhpur      Rajasthan Historical        9.10             0.722\n",
      "Hoshang Shah Tomb & Jami Masjid   Mandu Madhya Pradesh Historical        9.85             0.595\n",
      "                   Rumi Darwaza Lucknow  Uttar Pradesh Historical        9.60             0.583\n",
      "                     Jag Mandir Udaipur      Rajasthan Historical        9.20             0.568\n",
      "                 Blue City View Jodhpur      Rajasthan Historical        9.20             0.548\n",
      "\n",
      "‚úÖ Content-based recommendations working perfectly!\n"
     ]
    }
   ],
   "source": [
    "def get_content_recommendations(destination_name, cosine_sim, indices, df, top_n=10):\n",
    "    \"\"\"\n",
    "    Get content-based recommendations for a given destination\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    destination_name : str\n",
    "        Name of the destination\n",
    "    cosine_sim : numpy array\n",
    "        Cosine similarity matrix\n",
    "    indices : dict\n",
    "        Name to index mapping\n",
    "    df : DataFrame\n",
    "        Destinations dataframe\n",
    "    top_n : int\n",
    "        Number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with recommendations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get index\n",
    "        idx = indices.get(destination_name)\n",
    "        if idx is None:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N (excluding first one - itself)\n",
    "        sim_scores = sim_scores[1:top_n+1]\n",
    "        \n",
    "        # Get destination indices\n",
    "        dest_indices = [i[0] for i in sim_scores]\n",
    "        \n",
    "        # Create recommendations dataframe\n",
    "        recommendations = df.iloc[dest_indices][['Name', 'City', 'State/UT', 'Type', 'Popularity']].copy()\n",
    "        recommendations['Similarity_Score'] = [round(i[1], 3) for i in sim_scores]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test the function\n",
    "print(\"\\n‚úÖ Testing Content-Based Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_destination = df['Name'].iloc[0]\n",
    "print(f\"\\nüéØ Getting recommendations for: {test_destination}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "recommendations = get_content_recommendations(test_destination, cosine_sim, indices, df, top_n=5)\n",
    "\n",
    "if not recommendations.empty:\n",
    "    print(\"\\nTop 5 Similar Destinations:\")\n",
    "    print(recommendations.to_string(index=False))\n",
    "    print(\"\\n‚úÖ Content-based recommendations working perfectly!\")\n",
    "else:\n",
    "    print(\"‚ùå No recommendations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë• Step 10: Build Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë• Building Collaborative Filtering Model...\n",
      "================================================================================\n",
      "\n",
      "1. Creating User-Item Matrix...\n",
      "   Using ExperienceRating as interaction value\n",
      "\n",
      "‚úÖ User-Item Matrix Created!\n",
      "   Shape: (7275, 7420)\n",
      "   (7,275 users √ó 7,420 destinations)\n",
      "   Total ratings: 12,274\n",
      "   Sparsity: 99.98%\n",
      "   Average ratings per user: 1.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ User-Item Matrix Created!\n",
      "   Shape: (7275, 7420)\n",
      "   (7,275 users √ó 7,420 destinations)\n",
      "   Total ratings: 12,274\n",
      "   Sparsity: 99.98%\n",
      "   Average ratings per user: 1.7\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüë• Building Collaborative Filtering Model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Creating User-Item Matrix...\")\n",
    "print(\"   Using ExperienceRating as interaction value\")\n",
    "\n",
    "# Create user-item matrix from history data\n",
    "user_item_matrix = history_df.pivot_table(\n",
    "    index='UserID',\n",
    "    columns='DestinationID',\n",
    "    values='ExperienceRating',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ User-Item Matrix Created!\")\n",
    "print(f\"   Shape: {user_item_matrix.shape}\")\n",
    "print(f\"   ({user_item_matrix.shape[0]:,} users √ó {user_item_matrix.shape[1]:,} destinations)\")\n",
    "print(f\"   Total ratings: {(user_item_matrix > 0).sum().sum():,}\")\n",
    "\n",
    "# Calculate sparsity\n",
    "total_cells = user_item_matrix.shape[0] * user_item_matrix.shape[1]\n",
    "non_zero = (user_item_matrix != 0).sum().sum()\n",
    "sparsity = (1 - (non_zero / total_cells)) * 100\n",
    "print(f\"   Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"   Average ratings per user: {non_zero / user_item_matrix.shape[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Computing User Similarity Matrix...\n",
      "   ‚è≥ This may take a few minutes...\n",
      "\n",
      "\n",
      "‚úÖ User Similarity Matrix Created!\n",
      "   Shape: (7275, 7275)\n",
      "   (7,275 √ó 7,275)\n",
      "   Memory: ~403.79 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Computing User Similarity Matrix...\")\n",
    "print(\"   ‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "# Compute user similarity using cosine similarity\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "user_similarity_df = pd.DataFrame(\n",
    "    user_similarity,\n",
    "    index=user_item_matrix.index,\n",
    "    columns=user_item_matrix.index\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ User Similarity Matrix Created!\")\n",
    "print(f\"   Shape: {user_similarity_df.shape}\")\n",
    "print(f\"   ({user_similarity_df.shape[0]:,} √ó {user_similarity_df.shape[1]:,})\")\n",
    "print(f\"   Memory: ~{user_similarity_df.values.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 11: Test Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Testing Collaborative Filtering Recommendations:\n",
      "================================================================================\n",
      "\n",
      "üéØ Getting recommendations for User ID: 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 Recommended Destinations:\n",
      " DestinationID                      Name         City          State/UT       Type  Popularity  Predicted_Score\n",
      "          9396              Royal Konark       Konark            Odisha       Fort        7.23            5.126\n",
      "          5777                    Nagpur       Nagpur       Maharashtra       Lake        7.88            0.513\n",
      "           625        Martand Sun Temple     Anantnag   Jammu & Kashmir Historical        9.50            0.000\n",
      "          3429 Pin Valley to Mudh Winter         Mudh             Spiti    Offbeat        9.92            0.000\n",
      "          2041  Bhismaknagar Winter Fort Bhismaknagar Arunachal Pradesh Historical        9.70            0.000\n",
      "\n",
      "‚úÖ Collaborative filtering recommendations working perfectly!\n"
     ]
    }
   ],
   "source": [
    "def get_collaborative_recommendations(user_id, user_similarity_df, user_item_matrix, df, top_n=10):\n",
    "    \"\"\"\n",
    "    Get collaborative filtering recommendations for a user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        User ID\n",
    "    user_similarity_df : DataFrame\n",
    "        User similarity matrix\n",
    "    user_item_matrix : DataFrame\n",
    "        User-item interaction matrix\n",
    "    df : DataFrame\n",
    "        Destinations dataframe\n",
    "    top_n : int\n",
    "        Number of recommendations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with recommendations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if user_id not in user_similarity_df.index:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get similar users (top 10)\n",
    "        similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:11]\n",
    "        \n",
    "        # Get destinations rated by target user\n",
    "        user_rated = user_item_matrix.loc[user_id]\n",
    "        user_rated_dest = user_rated[user_rated > 0].index.tolist()\n",
    "        \n",
    "        # Aggregate ratings from similar users\n",
    "        recommendations = {}\n",
    "        for sim_user, similarity in similar_users.items():\n",
    "            sim_user_ratings = user_item_matrix.loc[sim_user]\n",
    "            for dest_id, rating in sim_user_ratings.items():\n",
    "                if rating > 0 and dest_id not in user_rated_dest:\n",
    "                    if dest_id not in recommendations:\n",
    "                        recommendations[dest_id] = 0\n",
    "                    recommendations[dest_id] += rating * similarity\n",
    "        \n",
    "        # Sort and get top N\n",
    "        top_dest = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        if not top_dest:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get destination details\n",
    "        dest_ids = [d[0] for d in top_dest]\n",
    "        result = df[df['DestinationID'].isin(dest_ids)][['DestinationID', 'Name', 'City', 'State/UT', 'Type', 'Popularity']].copy()\n",
    "        \n",
    "        # Add predicted scores\n",
    "        score_map = {d[0]: round(d[1], 3) for d in top_dest}\n",
    "        result['Predicted_Score'] = result['DestinationID'].map(score_map)\n",
    "        result = result.sort_values('Predicted_Score', ascending=False)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test the function\n",
    "print(\"\\n‚úÖ Testing Collaborative Filtering Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_user = user_item_matrix.index[0]\n",
    "print(f\"\\nüéØ Getting recommendations for User ID: {test_user}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "collab_recommendations = get_collaborative_recommendations(\n",
    "    test_user, user_similarity_df, user_item_matrix, df, top_n=5\n",
    ")\n",
    "\n",
    "if not collab_recommendations.empty:\n",
    "    print(\"\\nTop 5 Recommended Destinations:\")\n",
    "    print(collab_recommendations.to_string(index=False))\n",
    "    print(\"\\n‚úÖ Collaborative filtering recommendations working perfectly!\")\n",
    "else:\n",
    "    print(\"‚ùå No recommendations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 12: Save All Models and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving all models and files...\n",
      "================================================================================\n",
      "\n",
      "[1/5] Saving TF-IDF Vectorizer...\n",
      "      ‚úÖ tfidf_vectorizer.pkl (212.6 KB)\n",
      "\n",
      "[2/5] Saving Cosine Similarity Matrix...\n",
      "      ‚úÖ cosine_similarity.pkl (757.5 MB)\n",
      "\n",
      "[3/5] Saving Indices Mapping...\n",
      "      ‚úÖ indices.pkl (246.8 KB)\n",
      "\n",
      "[4/5] Saving User-Item Matrix...\n",
      "      ‚úÖ user_item_matrix.pkl (412.0 MB)\n",
      "\n",
      "[5/5] Saving User Similarity Matrix...\n",
      "      ‚úÖ user_similarity_df.pkl (403.8 MB)\n",
      "\n",
      "================================================================================\n",
      "üéâ ALL MODELS SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ Saving all models and files...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save TF-IDF Vectorizer\n",
    "print(\"\\n[1/5] Saving TF-IDF Vectorizer...\")\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "size = os.path.getsize('tfidf_vectorizer.pkl') / 1024\n",
    "print(f\"      ‚úÖ tfidf_vectorizer.pkl ({size:.1f} KB)\")\n",
    "\n",
    "# 2. Save Cosine Similarity Matrix\n",
    "print(\"\\n[2/5] Saving Cosine Similarity Matrix...\")\n",
    "with open('cosine_similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(cosine_sim, f)\n",
    "size = os.path.getsize('cosine_similarity.pkl') / (1024 * 1024)\n",
    "print(f\"      ‚úÖ cosine_similarity.pkl ({size:.1f} MB)\")\n",
    "\n",
    "# 3. Save Indices\n",
    "print(\"\\n[3/5] Saving Indices Mapping...\")\n",
    "with open('indices.pkl', 'wb') as f:\n",
    "    pickle.dump(indices, f)\n",
    "size = os.path.getsize('indices.pkl') / 1024\n",
    "print(f\"      ‚úÖ indices.pkl ({size:.1f} KB)\")\n",
    "\n",
    "# 4. Save User-Item Matrix\n",
    "print(\"\\n[4/5] Saving User-Item Matrix...\")\n",
    "user_item_matrix.to_pickle('user_item_matrix.pkl')\n",
    "size = os.path.getsize('user_item_matrix.pkl') / (1024 * 1024)\n",
    "print(f\"      ‚úÖ user_item_matrix.pkl ({size:.1f} MB)\")\n",
    "\n",
    "# 5. Save User Similarity DataFrame\n",
    "print(\"\\n[5/5] Saving User Similarity Matrix...\")\n",
    "user_similarity_df.to_pickle('user_similarity_df.pkl')\n",
    "size = os.path.getsize('user_similarity_df.pkl') / (1024 * 1024)\n",
    "print(f\"      ‚úÖ user_similarity_df.pkl ({size:.1f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 13: Verification and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION - Checking All Required Files\n",
      "================================================================================\n",
      "\n",
      "üì¶ Model Files:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚úÖ tfidf_vectorizer.pkl           (0.21 MB)\n",
      "   ‚úÖ cosine_similarity.pkl          (757.46 MB)\n",
      "   ‚úÖ indices.pkl                    (0.24 MB)\n",
      "   ‚úÖ user_item_matrix.pkl           (411.95 MB)\n",
      "   ‚úÖ user_similarity_df.pkl         (403.85 MB)\n",
      "\n",
      "üìä Data Files:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚úÖ Users_df.csv                   (1.63 MB)\n",
      "   ‚úÖ Destination_df.csv             (7.59 MB)\n",
      "   ‚úÖ Users_History_df.csv           (1.34 MB)\n",
      "   ‚úÖ Reviews_df.csv                 (1.39 MB)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SUCCESS! All required files are present and ready.\n",
      "\n",
      "üì± You can now run the Streamlit application:\n",
      "\n",
      "   streamlit run app.py\n",
      "\n",
      "üí° The app will load all models automatically!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION - Checking All Required Files\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required_files = [\n",
    "    'tfidf_vectorizer.pkl',\n",
    "    'cosine_similarity.pkl',\n",
    "    'indices.pkl',\n",
    "    'user_item_matrix.pkl',\n",
    "    'user_similarity_df.pkl'\n",
    "]\n",
    "\n",
    "data_files = [\n",
    "    'Users_df.csv',\n",
    "    'Destination_df.csv',\n",
    "    'Users_History_df.csv',\n",
    "    'Reviews_df.csv'\n",
    "]\n",
    "\n",
    "print(\"\\nüì¶ Model Files:\")\n",
    "print(\"-\"*80)\n",
    "all_models_present = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {file:<30} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file:<30} NOT FOUND\")\n",
    "        all_models_present = False\n",
    "\n",
    "print(\"\\nüìä Data Files:\")\n",
    "print(\"-\"*80)\n",
    "all_data_present = True\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {file:<30} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file:<30} NOT FOUND\")\n",
    "        all_data_present = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if all_models_present and all_data_present:\n",
    "    print(\"‚úÖ SUCCESS! All required files are present and ready.\")\n",
    "    print(\"\\nüì± You can now run the Streamlit application:\")\n",
    "    print(\"\\n   streamlit run app.py\")\n",
    "    print(\"\\nüí° The app will load all models automatically!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Some files are missing. Please check above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 14: Model Statistics and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL STATISTICS AND PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Total Destinations: 9,964\n",
      "   ‚Ä¢ Total Users: 10,000\n",
      "   ‚Ä¢ Total Trips: 12,275\n",
      "   ‚Ä¢ Total Reviews: 10,000\n",
      "   ‚Ä¢ Destination Types: 44\n",
      "   ‚Ä¢ States/UTs: 165\n",
      "\n",
      "ü§ñ Content-Based Filtering Model:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ TF-IDF Features: 5,000\n",
      "   ‚Ä¢ Destinations Covered: 9,866\n",
      "   ‚Ä¢ Similarity Matrix Size: 9,964 √ó 9,964\n",
      "   ‚Ä¢ Feature Sparsity: 99.26%\n",
      "\n",
      "üë• Collaborative Filtering Model:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Active Users: 7,275\n",
      "   ‚Ä¢ Rated Destinations: 7,420\n",
      "   ‚Ä¢ Total Ratings: 12,274\n",
      "   ‚Ä¢ Matrix Sparsity: 99.98%\n",
      "   ‚Ä¢ Avg Ratings per User: 1.7\n",
      "   ‚Ä¢ User Similarity Matrix: 7,275 √ó 7,275\n",
      "\n",
      "üèÜ Top Destination Types:\n",
      "--------------------------------------------------------------------------------\n",
      "    1. Nature                    :  1629 destinations\n",
      "    2. Adventure                 :  1421 destinations\n",
      "    3. Wildlife                  :   698 destinations\n",
      "    4. Heritage                  :   598 destinations\n",
      "    5. Religious                 :   564 destinations\n",
      "    6. Historical                :   512 destinations\n",
      "    7. Beach                     :   500 destinations\n",
      "    8. Museum                    :   434 destinations\n",
      "    9. Hill Station              :   433 destinations\n",
      "   10. Temple                    :   418 destinations\n",
      "\n",
      "üó∫Ô∏è  Top States/UTs:\n",
      "--------------------------------------------------------------------------------\n",
      "    1. West Bengal               :   661 destinations\n",
      "    2. Sikkim                    :   529 destinations\n",
      "    3. Ladakh                    :   504 destinations\n",
      "    4. Tamil Nadu                :   500 destinations\n",
      "    5. Himachal Pradesh          :   475 destinations\n",
      "    6. Uttarakhand               :   472 destinations\n",
      "    7. Maharashtra               :   422 destinations\n",
      "    8. Karnataka                 :   391 destinations\n",
      "    9. Kerala                    :   320 destinations\n",
      "   10. Meghalaya                 :   308 destinations\n",
      "\n",
      "================================================================================\n",
      "‚úÖ NOTEBOOK EXECUTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéâ All models trained and saved successfully!\n",
      "üì± Ready to deploy: streamlit run app.py\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL STATISTICS AND PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   ‚Ä¢ Total Destinations: {len(destinations_df):,}\")\n",
    "print(f\"   ‚Ä¢ Total Users: {len(users_df):,}\")\n",
    "print(f\"   ‚Ä¢ Total Trips: {len(history_df):,}\")\n",
    "print(f\"   ‚Ä¢ Total Reviews: {len(reviews_df):,}\")\n",
    "print(f\"   ‚Ä¢ Destination Types: {destinations_df['Type'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ States/UTs: {destinations_df['State/UT'].nunique()}\")\n",
    "\n",
    "print(\"\\nü§ñ Content-Based Filtering Model:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   ‚Ä¢ TF-IDF Features: {tfidf_matrix.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Destinations Covered: {len(indices):,}\")\n",
    "print(f\"   ‚Ä¢ Similarity Matrix Size: {cosine_sim.shape[0]:,} √ó {cosine_sim.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Feature Sparsity: {(1.0 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nüë• Collaborative Filtering Model:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   ‚Ä¢ Active Users: {user_item_matrix.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Rated Destinations: {user_item_matrix.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Total Ratings: {(user_item_matrix > 0).sum().sum():,}\")\n",
    "total_cells = user_item_matrix.shape[0] * user_item_matrix.shape[1]\n",
    "non_zero = (user_item_matrix != 0).sum().sum()\n",
    "print(f\"   ‚Ä¢ Matrix Sparsity: {(1 - (non_zero / total_cells)) * 100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Avg Ratings per User: {non_zero / user_item_matrix.shape[0]:.1f}\")\n",
    "print(f\"   ‚Ä¢ User Similarity Matrix: {user_similarity_df.shape[0]:,} √ó {user_similarity_df.shape[1]:,}\")\n",
    "\n",
    "print(\"\\nüèÜ Top Destination Types:\")\n",
    "print(\"-\"*80)\n",
    "top_types = destinations_df['Type'].value_counts().head(10)\n",
    "for i, (dtype, count) in enumerate(top_types.items(), 1):\n",
    "    print(f\"   {i:2d}. {dtype:<25} : {count:>5} destinations\")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è  Top States/UTs:\")\n",
    "print(\"-\"*80)\n",
    "top_states = destinations_df['State/UT'].value_counts().head(10)\n",
    "for i, (state, count) in enumerate(top_states.items(), 1):\n",
    "    print(f\"   {i:2d}. {state:<25} : {count:>5} destinations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NOTEBOOK EXECUTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéâ All models trained and saved successfully!\")\n",
    "print(\"üì± Ready to deploy: streamlit run app.py\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
